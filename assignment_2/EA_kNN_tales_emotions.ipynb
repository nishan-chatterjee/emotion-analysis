{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances before removing classes:  13693\n",
      "Instanecs after removing classes:  4263\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../../tales_emotion.tsv', sep='\\t', header=0)\n",
    "print(\"Instances before removing classes: \", len(data))\n",
    "emotions = [\"fear\", \"anger\",\"disgust\",\"sadness\",\"joy\"]\n",
    "data = data[data.emotion_label != \"surprise\"]\n",
    "data = data[data.emotion_label != \"noemo\"]\n",
    "print(\"Instanecs after removing classes: \",len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = data[\"emotion_label\"]\n",
    "text_data = data[\"text\"]\n",
    "assert(len(label_data) == len(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text_data = [word_tokenize(x) for x in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listen to the story of Jemima Puddle-duck, who was annoyed because the farmer's wife would not let her hatch her own eggs.\n",
      "anger\n",
      "['Her', 'sister-in-law', ',', 'Mrs.', 'Rebeccah', 'Puddle-duck', ',', 'was', 'perfectly', 'willing', 'to', 'leave', 'the', 'hatching', 'to', 'someone', 'else', '--', '``', 'I', 'have', 'not', 'the', 'patience', 'to', 'sit', 'on', 'a', 'nest', 'for', 'twenty-eight', 'days', ';', 'and', 'no', 'more', 'have', 'you', ',', 'Jemima', '.']\n"
     ]
    }
   ],
   "source": [
    "print(text_data[1])\n",
    "print(label_data[1])\n",
    "print(tokenized_text_data[1])\n",
    "labels = set(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NRC = pd.read_csv('../../NRCLexicon.csv', sep='\\t', header=0)\n",
    "newNRC = NRC.groupby(['word']).agg(lambda x: tuple(x)).applymap(list).reset_index() #because words have multiple emotions, want list of emotions per word\n",
    "wordAssociations = dict(zip(newNRC['word'], newNRC['emotion'])) # this is the database that you want to use for feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "Which features do we want to extract?\n",
    "\n",
    "    - count of emotions associated with words in text \n",
    "    \n",
    "        [fear, anger, trust, sadness, surprise, joy, anticipation, disgust] --> 8\n",
    "    \n",
    "    - count of valence \n",
    "    \n",
    "        [positive, negative]  --> 2\n",
    "        \n",
    "    - length of sentence (in words)  --> 1\n",
    "    \n",
    "    - length of sentence (in chars)  --> 1\n",
    "    \n",
    "    - count of punctuation\n",
    "    \n",
    "        [',', '.', '?', '!', '#', '-', ';', ':']  --> 8\n",
    "    \n",
    "    - tense\n",
    "    \n",
    "        [future, present, past]  --> 3\n",
    "        \n",
    "    - negation count  --> 1\n",
    "\n",
    "    - count of capital letters --> 1\n",
    "    \n",
    "    - rate of repitition of words  --> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "def determine_tense(sentence): #sentence should be a list of tokens\n",
    "    tagged = pos_tag(sentence)\n",
    "\n",
    "    tense = [0,0,0]\n",
    "    tense[0] = len([word for word in tagged if word[1] in [\"MD\", \"VBC\", \"VBF\"]])  #future\n",
    "    tense[1] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])  #present\n",
    "    tense[2] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]])        #past\n",
    "    return tense\n",
    "\n",
    "print(determine_tense([\"This\", \"is\", \"a\", \"fine\",\"day\",\".\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 0, 0, 1, 0, 1, 2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "def count_emotions(text): #text should be a list of tokens\n",
    "    #TODO: check if each word in text (tokenized sentence) is in NRC, if so, add 1 to count for that emotion\n",
    "    #should also check for valence :) so this vector will be length 10 (8 emotions, 2 valence)\n",
    "    emotion_count = {\"positive\":0,\"negative\":0,\"fear\":0, \"anger\":0, \"trust\":0, \"sadness\":0, \"surprise\":0, \"joy\":0, \"anticipation\":0, \"disgust\":0\n",
    "}\n",
    "    for w in text:\n",
    "        if w.lower() in wordAssociations.keys():\n",
    "            for e in wordAssociations[w.lower()]:\n",
    "                emotion_count[e] += 1\n",
    "    return list(emotion_count.values())\n",
    "\n",
    "print(count_emotions([\"I\",\"love\",\"the\",\"sun\",\"on\",\"my\",\"smooth\",\"face\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def count_punctuation(text): #text must be list of tokens\n",
    "    #TODO: count all punctuation in the text\n",
    "    punctuation = [',', '.', '?', '!', '#', '-', ';', ':']\n",
    "    punctuation_count = {',':0, '.':0, '?':0, '!':0, '#':0, '-':0, ';':0, ':':0}\n",
    "    for w in text:\n",
    "        if w in punctuation:\n",
    "            punctuation_count[w] += 1\n",
    "    return list(punctuation_count.values())\n",
    "\n",
    "print(count_punctuation([\"This\",\"!\", \"is\", \"a\", \"fine\",\"day\",\".\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def count_negation(text): #text must be list of tokens\n",
    "    count = 0\n",
    "    negs = [\"not\",\"never\",\"no\",\"none\",\"neither\", \"nor\"]\n",
    "    for w in text:\n",
    "        if w.lower() in negs:\n",
    "            count += 1\n",
    "    return count #returns single int\n",
    "\n",
    "print(count_negation([\"This\", \"is\",\"not\", \"a\", \"fine\",\"day\",\".\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "def count_capitals(text): #takes a list of tokens\n",
    "    text = ' '.join(text)\n",
    "    return(sum(1 for c in text if c.isupper()))  #returns a single int\n",
    "\n",
    "print(count_capitals([\"This\", \"is\",\"NOT\", \"a\", \"FINE\",\"day\",\".\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def calc_rep_rate(text):\n",
    "    uniq = set(text)\n",
    "    return(int(round(len(text)/len(uniq), 0)))\n",
    "\n",
    "print(calc_rep_rate([\"This\", \"is\",\"not\", \"a\", \"fine\",\"day\",\",\",\"its\",\"a\", \"fine\",\"day\",\".\",\".\",\".\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 2]\n",
      "[2, 0, 0, 0, 1, 0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def create_feature_vector(text): #text must be a list of tokens\n",
    "    #TODO: call all the methods that will process the text\n",
    "    #count_emotions (also valence)\n",
    "    #len(sentence)\n",
    "    #len(sentenceChars)\n",
    "    #count_punctuation\n",
    "    #determine_tense\n",
    "    #count_negation\n",
    "    #count_capitals\n",
    "    #calculate_repetition\n",
    "    feat_vec = []\n",
    "    feat_vec += count_emotions(text)\n",
    "    #feat_vec.append(len(text))\n",
    "    #feat_vec.append(len(' '.join(text)))\n",
    "    feat_vec += count_punctuation(text)\n",
    "    feat_vec += determine_tense(text)\n",
    "    feat_vec.append(count_negation(text))\n",
    "    feat_vec.append(count_capitals(text))\n",
    "    feat_vec.append(calc_rep_rate(text))\n",
    "    assert len(feat_vec) == 24\n",
    "    return feat_vec\n",
    "\n",
    "print(create_feature_vector([\"This\", \"is\",\"not\", \"a\", \"fine\",\"day\",\",\",\"its\",\"a\", \"fine\",\"day\",\".\",\".\",\".\"]))\n",
    "print(create_feature_vector([\"I\",\"love\",\"the\",\"sun\",\"on\",\"my\",\"smooth\",\"face\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing on Tales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 39   5  21  31  12]\n",
      " [ 10   1   9  12  11]\n",
      " [ 29   4  24  28  17]\n",
      " [ 25   9  23 182  20]\n",
      " [ 32   6  17  46  27]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.29      0.36      0.32       108\n",
      "     disgust       0.04      0.02      0.03        43\n",
      "        fear       0.26      0.24      0.24       102\n",
      "         joy       0.61      0.70      0.65       259\n",
      "     sadness       0.31      0.21      0.25       128\n",
      "\n",
      "    accuracy                           0.43       640\n",
      "   macro avg       0.30      0.31      0.30       640\n",
      "weighted avg       0.40      0.43      0.41       640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "dataset = []\n",
    "for instance in tokenized_text_data:\n",
    "    dataset.append(create_feature_vector(instance))\n",
    "\n",
    "X = dataset\n",
    "y = label_data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)  #split dataset into 85% train and 15% test\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) #feature scaling\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels=[\"anger\",\"disgust\",\"fear\",\"joy\",\"sadness\"]))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639.4499999999999"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = len(dataset)*0.15\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on ISEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instances before removing classes:  7666\n",
      "Instances after removing classes:  5477\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('../../isear_emotion.tsv', sep='\\t', header=0)\n",
    "print(\"Instances before removing classes: \", len(data))\n",
    "data = data[data.emotion_label != \"shame\"]\n",
    "data = data[data.emotion_label != \"guilt\"]\n",
    "print(\"Instances after removing classes: \",len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = data[\"emotion_label\"]\n",
    "text_data = data[\"text\"]\n",
    "assert(len(label_data) == len(text_data))\n",
    "tokenized_text_data = [word_tokenize(x) for x in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When I was involved in a traffic accident.\n",
      "fear\n",
      "['When', 'I', 'was', 'involved', 'in', 'a', 'traffic', 'accident', '.']\n"
     ]
    }
   ],
   "source": [
    "print(text_data[1])\n",
    "print(label_data[1])\n",
    "print(tokenized_text_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "639\n",
      "[[  8   4  17  79  15]\n",
      " [ 10   2  15  91  15]\n",
      " [  4   3  25  80  11]\n",
      " [  0   2   5 132   7]\n",
      " [  5   4  19  68  18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.30      0.07      0.11       123\n",
      "     disgust       0.13      0.02      0.03       133\n",
      "        fear       0.31      0.20      0.25       123\n",
      "         joy       0.29      0.90      0.44       146\n",
      "     sadness       0.27      0.16      0.20       114\n",
      "\n",
      "    accuracy                           0.29       639\n",
      "   macro avg       0.26      0.27      0.20       639\n",
      "weighted avg       0.26      0.29      0.21       639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "for instance in tokenized_text_data:\n",
    "    dataset.append(create_feature_vector(instance))\n",
    "\n",
    "X = dataset\n",
    "y = label_data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1165)  #select the same # of instances as were tested for Tales above\n",
    "print(len(X_test))\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels=[\"anger\",\"disgust\",\"fear\",\"joy\",\"sadness\"]))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
